{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import API\n",
    "from tweepy import Cursor\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import collections\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from array import array\n",
    "import math\n",
    "import operator\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.read_json('file.json', orient ='split', compression = 'infer') \n",
    "df_tweets = df_tweets.fillna(value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getrts(df_tweets):\n",
    "    retweets_ = df_tweets[\"retweeted_status\"].apply(lambda x:0 if str(x)=='nan' else 1)\n",
    "    return retweets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We recieve a list of 0 if tweet is unique and 1 if it is a retweet\n",
    "retweets = getrts(df_tweets)\n",
    "#We get the index of the retweets\n",
    "retweets = retweets[retweets == 1]\n",
    "retweets = retweets.index.values\n",
    "#We now get df_unique which are the original tweets and df_retweeted which are the retweeted tweets\n",
    "df_unique = df_tweets.drop(retweets,axis=0)\n",
    "df_unique = df_unique.reset_index(drop=True)\n",
    "df_retweeted = df_tweets.loc[retweets]\n",
    "df_retweeted = df_retweeted[df_retweeted['retweeted_status'].notna()]\n",
    "df_retweeted = df_retweeted.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We get common columns between the retweeted and the original tweets \n",
    "cols = df_retweeted.columns&df_retweeted.at[0,'retweeted_status'].keys()\n",
    "\n",
    "for i in range(0,len(df_retweeted)):\n",
    "    for col in cols:\n",
    "        if col in df_retweeted.at[i,'retweeted_status'].keys():\n",
    "            #Substitute the columns of the retweeted tweet by the original one\n",
    "            if col != 'possibly_sensitive':\n",
    "                df_retweeted.at[i,col] = df_retweeted.at[i,'retweeted_status'][col] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.concat([df_unique, df_retweeted], ignore_index=True)\n",
    "df_tweets = df_tweets.drop_duplicates(subset=['id'])\n",
    "df_tweets = df_tweets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [None]*len(df_tweets)\n",
    "for i in range(0,len(df_tweets)):\n",
    "    \n",
    "    if type(df_tweets.at[i,'extended_tweet']) != float:\n",
    "        lines[i] = df_tweets.at[i,'extended_tweet']['full_text']\n",
    "    \n",
    "    else:\n",
    "        lines[i] = df_tweets.at[i,'text']\n",
    "\n",
    "lines = [l.strip() for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(text):\n",
    "    \n",
    "    result = text.split()\n",
    "    aux = text.split()\n",
    "    for word in aux:\n",
    "        if '@' in word:\n",
    "            result.remove(word)\n",
    "        elif 'https' in word:\n",
    "            result.remove(word)\n",
    "    result = ' '.join(result)\n",
    "    \n",
    "    return deEmojify(result)\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "def delete_hashtags(text):\n",
    "    \n",
    "    result = text.split()\n",
    "    aux = text.split()\n",
    "    for word in aux:\n",
    "        if '#' in word:\n",
    "            result.remove(word)\n",
    "    result = ' '.join(result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def getTerms(line):\n",
    "    \n",
    "    #We remove mentions (@username) and links\n",
    "    line = remove_mentions(line)\n",
    "    #We remove Hashtags\n",
    "    line = delete_hashtags(line)\n",
    "    #We will get rid of RTs\n",
    "    line= line.replace(\"RT \", \"\").strip()\n",
    "    #We remove punctuations\n",
    "    line=  line.lower() \n",
    "    line = re.sub(r'[^\\w\\s]','',line).strip()\n",
    "    #We remove stop-words\n",
    "    line=  line.split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    line=[word for word in line if word not in stops] \n",
    "    #Stemming\n",
    "    stemming = PorterStemmer()\n",
    "    line=[stemming.stem(word) for word in line] \n",
    "    \n",
    "    return line\n",
    "\n",
    "def create_index_tfidf(lines, numDocuments):\n",
    "        \n",
    "    index=defaultdict(list)\n",
    "    \n",
    "    #Term frequencies of terms in documents (tweets)\n",
    "    tf=defaultdict(list) \n",
    "    \n",
    "    #Document (tweet) frequencies of terms in the corpus\n",
    "    df=defaultdict(int)         \n",
    "    \n",
    "    #Inverse document (tweet) frequencies\n",
    "    idf=defaultdict(float)\n",
    "    \n",
    "    #Count we will use to assign IDs to tweets\n",
    "    count = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        \n",
    "        tweet_id = count\n",
    "        \n",
    "        #We get the terms of the tweet\n",
    "        terms = getTerms(line)                     \n",
    "        termdictPage={}\n",
    "\n",
    "        for position, term in enumerate(terms): \n",
    "            try:\n",
    "                #If the term is already in the dict append the position to the corrisponding list\n",
    "                termdictPage[term][1].append(position) \n",
    "            except:\n",
    "                #Add the new term as dict key and initialize the array of positions and add the position\n",
    "                termdictPage[term]=[tweet_id, array('I',[position])]\n",
    "        \n",
    "        #Normalize term frequencies. Norm is the same for all terms of a document.\n",
    "        norm=0\n",
    "        for term, posting in termdictPage.items(): \n",
    "            norm+=len(posting[1])**2\n",
    "        norm=math.sqrt(norm)\n",
    "        \n",
    "        #Calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in termdictPage.items():     \n",
    "            tf[term].append(np.round(len(posting[1])/norm,4))  \n",
    "            df[term] += 1\n",
    "        \n",
    "        #Merge the current page index with the main index\n",
    "        for termpage, postingpage in termdictPage.items():\n",
    "            index[termpage].append(postingpage)\n",
    "        \n",
    "        count += 1\n",
    "    \n",
    "    #Compute idf\n",
    "    for term in df:\n",
    "        idf[term] = np.round(np.log(float(numDocuments/df[term])),4)\n",
    "           \n",
    "    return index, tf, df, idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 263.88 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "numDocuments = len(lines)\n",
    "index, tf, df, idf = create_index_tfidf(lines, numDocuments)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankDocuments(terms, docs, index, idf, tf):\n",
    "        \n",
    "    #We are interested only on the element of the docVector corresponding to the query terms, so remaining elemnts\n",
    "    #would became 0 when multiplied to the queryVector\n",
    "    docVectors=defaultdict(lambda: [0]*len(terms)) \n",
    "    queryVector=[0]*len(terms)    \n",
    "    \n",
    "    #Get frequency of each term in the query\n",
    "    query_terms_count = collections.Counter(terms)  \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "    \n",
    "    for termIndex, term in enumerate(terms): \n",
    "        if term not in index:\n",
    "            continue\n",
    "            \n",
    "        #Compute tf*idf\n",
    "        queryVector[termIndex]=query_terms_count[term]/query_norm * idf[term] \n",
    "        \n",
    "        # Generate docVectors for matching docs\n",
    "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
    "            if doc in docs:\n",
    "                docVectors[doc][termIndex]=tf[term][docIndex] * idf[term]  # Check if multiply for idf\n",
    "    \n",
    "    #Calculate the score of each doc computing cosine similarity between queryVector and each docVector\n",
    "    docScores=[ [np.dot(curDocVec, queryVector), doc] for doc, curDocVec in docVectors.items() ]\n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[x[1] for x in docScores]\n",
    "    \n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found\")\n",
    "        \n",
    "    return resultDocs\n",
    "\n",
    "def search_tf_idf(query, index):\n",
    "    \n",
    "    #Get terms of the query\n",
    "    query=getTerms(query)\n",
    "    docs=set()\n",
    "    \n",
    "    #Count will help us to avoid a problem when doing instersection\n",
    "    count = 0\n",
    "    \n",
    "    for term in query:\n",
    "        try:\n",
    "            #Store in termDocs the ids of the docs that contain \"term\"\n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            #As tweet must contain ALL word in query docs = docs Instersection termDocs\n",
    "            if count == 0:\n",
    "                docs |= set(termDocs)\n",
    "                count += 1\n",
    "            else:\n",
    "                docs &= set(termDocs)\n",
    "        except:\n",
    "            #Term not in index\n",
    "            pass\n",
    "        \n",
    "    docs=list(docs)\n",
    "    #We rank documents with TF-IDF\n",
    "    ranked_docs = rankDocuments(query, docs, index, idf, tf) \n",
    "    \n",
    "    return ranked_docs\n",
    "\n",
    "def nolinks(text):\n",
    "    text = text.split()\n",
    "    aux = text.copy()\n",
    "    for word in aux:\n",
    "        if 'https' in word:\n",
    "            text.remove(word)\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "def PrintTweets(ranked_docs,top):\n",
    "    \n",
    "    print(\"\\n======================\\nTop {} results out of {} for the seached query:\\n\".format(top, len(ranked_docs)))\n",
    "    count = 0\n",
    "    for d_id in ranked_docs[:top] :\n",
    "        \n",
    "        print('Result number:',count+1,'\\n')\n",
    "\n",
    "        if type(df_tweets.at[d_id,'extended_tweet']) == float:\n",
    "            print('Tweet: ',nolinks(df_tweets.at[d_id,'text']))\n",
    "        else:\n",
    "            print('Tweet: ',nolinks(df_tweets.at[d_id,'extended_tweet']['full_text']))\n",
    "\n",
    "        print('\\nUsername: ',df_tweets.at[d_id,'user']['screen_name'])\n",
    "\n",
    "        print('\\nDate: ')\n",
    "\n",
    "        if df_tweets.at[d_id,\"entities\"]['hashtags'] != []:\n",
    "            hashs = [None]*len(df_tweets.at[d_id,\"entities\"]['hashtags'])\n",
    "            for i in range(0,len(df_tweets.at[d_id,\"entities\"]['hashtags'])):\n",
    "                hashs[i] = df_tweets.at[d_id,\"entities\"]['hashtags'][i]['text']\n",
    "            print('\\nHashtags: ',hashs)\n",
    "        else:\n",
    "            print('\\nHashtags: None')\n",
    "\n",
    "        print('\\nLikes: ',df_tweets.at[d_id,'favorite_count'])\n",
    "\n",
    "        print('\\nRetweets: ',df_tweets.at[d_id,'retweet_count'])\n",
    "\n",
    "        print('\\nURL: https://twitter.com/twitter/statuses/'+str(df_tweets.at[d_id,'id']))\n",
    "\n",
    "        print('\\n----------------------\\n')\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cluster_index.txt\", \"rb\") as fp:   # Unpickling\n",
    "    cluster_index = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output diversification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(df_tweets)):\n",
    "    df_tweets.at[i,'cluster'] = cluster_index[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a diversity score which the aim is to DIVERSIFY the final output. This score is assigned to the list of returned documents for the input query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diversify(top,n_cluster,ranked_docs_cluster):\n",
    "    optimal_freq = top/n_cluster  \n",
    "    c = [None]*n_cluster\n",
    "    for i in ranked_docs_cluster.cluster.unique():\n",
    "        c[int(i)] = ranked_docs_cluster[ranked_docs_cluster['cluster']==i][:round(optimal_freq)] \n",
    "    output = pd.concat([c[0],c[1]])\n",
    "    for i in range(2,len(c)):\n",
    "        output = pd.concat([output,c[i]])\n",
    "    output.sort_index(inplace=True)\n",
    "    return output['d_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClusterDiver(ranked_docs,top):\n",
    "    \n",
    "    ranked_docs_cluster = []\n",
    "\n",
    "    for d_id in ranked_docs: \n",
    "        ranked_docs_cluster.append(df_tweets.at[d_id,'cluster'])\n",
    "\n",
    "    ranked_docs_cluster = {'d_id':ranked_docs,'cluster':ranked_docs_cluster}\n",
    "    ranked_docs_cluster = pd.DataFrame(ranked_docs_cluster)\n",
    "    \n",
    "    ranked_docs = diversify(top,len(ranked_docs_cluster.cluster.unique()),ranked_docs_cluster)\n",
    "    \n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintTweetsCluster(ranked_docs):\n",
    "    for d_id in ranked_docs:\n",
    "        if type(df_tweets.at[d_id,'extended_tweet']) == float:\n",
    "            print(nolinks(df_tweets.at[d_id,'text']),'Cluster: ',df_tweets.at[d_id,'cluster'])\n",
    "            print('\\n')\n",
    "        else:\n",
    "            print(nolinks(df_tweets.at[d_id,'extended_tweet']['full_text']),'Cluster: ',df_tweets.at[d_id,'cluster'])\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "lockdown is so boring\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'search_tf_idf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-247d566e1c5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mranked_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_tf_idf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mranked_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClusterDiver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranked_docs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'search_tf_idf' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "top = 20\n",
    "n_clusters = 5\n",
    "\n",
    "ranked_docs = search_tf_idf(query, index)\n",
    "ranked_docs = ClusterDiver(ranked_docs,top)\n",
    "print('\\n')\n",
    "PrintTweetsCluster(ranked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
